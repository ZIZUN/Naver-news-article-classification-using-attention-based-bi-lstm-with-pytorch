{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import date, timedelta\n",
    "import urllib.request as req\n",
    "import re\n",
    "\n",
    "category = {'0':100, '1':101, '2':102, '3':103, '4':104, '5':105}\n",
    "\n",
    "base_url = 'https://news.naver.com/main/ranking/popularDay.nhn?rankingType=popular_day&sectionId={}&date={}'\n",
    "\n",
    "def download(data_per_category: int, csv_path: str, last_date: str = '20180918'):\n",
    "    opener = req.build_opener()\n",
    "    dataset = []\n",
    "    progress = 0\n",
    "    for keyword in category:\n",
    "        date = last_date\n",
    "        spare = data_per_category\n",
    "        while spare > 0:\n",
    "            date = __date_decrease__(date)\n",
    "            #print(date)\n",
    "            f = opener.open(base_url.format(category[keyword],date))\n",
    "            #print(f.read)\n",
    "            soup = BeautifulSoup(f, 'html.parser')\n",
    "            li = soup.select(\"ol[class='ranking_list'] > li > div >  a\")\n",
    "            # print('data set count: {}'.format(len(li)))\n",
    "\n",
    "            #dataset += [{'class': keyword, 'text': __clean_text__(t['href'])} for t in li]\n",
    "            for t in li:\n",
    "                text = ('https://news.naver.com') + (t['href'])\n",
    "                #print(text)\n",
    "\n",
    "                g = opener.open(text)\n",
    "                soup = BeautifulSoup(g, 'html.parser')\n",
    "                li2 = soup.select(\"div[class='article_body _font_setting_target size3 font1']\")\n",
    "\n",
    "                text=''\n",
    "                for item in soup.find_all('div', id='articleBodyContents'):\n",
    "                    text = text + str(item.find_all(text=True))\n",
    "\n",
    "                dataset += [{'class': keyword, 'text': __clean_text__(text)}]\n",
    "\n",
    "                    #데이터셋에 기사내용과 분류 추가\n",
    "            try:\n",
    "                progress = (100 / (data_per_category * 6) * len(dataset))\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            dataframe = pd.DataFrame(dataset)\n",
    "            dataframe.to_csv(csv_path,index=False)\n",
    "            if spare // 100 == 1:\n",
    "                print('[Progress] {} % download complete'.format(progress))\n",
    "            spare -= len(li)\n",
    "\n",
    "\n",
    "    print('[Success] All download was complete')\n",
    "\n",
    "def __date_decrease__(news_date : str):\n",
    "    t = time.strptime(news_date, '%Y%m%d')\n",
    "    newdate = date(t.tm_year, t.tm_mon, t.tm_mday) - timedelta(1)\n",
    "    return newdate.strftime('%Y%m%d')\n",
    "\n",
    "def __clean_text__(text):\n",
    "    text = re.sub('[a-zA-Z]', '', text)\n",
    "    pattern = '([a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+)'  # E-mail제거\n",
    "    text = re.sub(pattern=pattern, repl='', string=text)\n",
    "    pattern = '(http|ftp|https)://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+'  # URL제거\n",
    "    text = re.sub(pattern=pattern, repl='', string=text)\n",
    "    pattern = '([ㄱ-ㅎㅏ-ㅣ]+)'  # 한글 자음, 모음 제거\n",
    "    text = re.sub(pattern=pattern, repl='', string=text)\n",
    "    pattern = '<[^>]*>'  # HTML 태그 제거\n",
    "    text = re.sub(pattern=pattern, repl='', string=text)\n",
    "    pattern = '[^\\w\\s]'  # 특수기호제거\n",
    "    text = re.sub(pattern=pattern, repl='', string=text)\n",
    "    text = text.replace('본문 내용', '')\n",
    "    text = text.replace('플레이어', '',2)\n",
    "    text = text.replace('동영상 뉴스', '')\n",
    "    text = text.replace('오류를 우회하기 위한 함수 추가', '')\n",
    "    text = text[19:-300]\n",
    "    cleaned_text = re.sub('[\\{\\}\\[\\]\\/?→.▲※ㆍ＜◀▶◆,☞●;↓◇△《■】Δ▷:|Δ\\●)*~ⓒ━©》`!^＞【\\-_+<>@\\#$%&\\\\\\=\\(\\'\\\"]',' ', text)\n",
    "    return cleaned_text\n",
    "\n",
    "\n",
    "download(1000,'/news__data1.csv','20191101')\n",
    "download(1000,'/news__data2.csv','20191001')\n",
    "download(1000,'/news__data3.csv','20190901')\n",
    "download(1000,'/news__data4.csv','20190801')\n",
    "download(1000,'/news__data5.csv','20190701')\n",
    "download(1000,'/news__data6.csv','20190601')\n",
    "download(1000,'/news__data7.csv','20190501')\n",
    "download(1000,'/news__data8.csv','20190401')\n",
    "download(1000,'/news__data9.csv','20190301')\n",
    "download(1000,'/news__data10.csv','20190201')\n",
    "download(1000,'/news__data11.csv','20190101')\n",
    "download(1000,'/news__data12.csv','20181201')\n",
    "download(1000,'/news__data13.csv','20181101')\n",
    "download(1000,'/news__data14.csv','20181001')\n",
    "download(1000,'/news__data15.csv','20180901')\n",
    "download(1000,'/news__data16.csv','20180801')\n",
    "download(1000,'/news__data17.csv','20180701')\n",
    "download(1000,'/news__data18.csv','20180601')\n",
    "download(1000,'/news__data19.csv','20180501')\n",
    "download(1000,'/news__data20.csv','20180401')\n",
    "download(1000,'/news__data21.csv','20180301')\n",
    "download(1000,'/news__data22.csv','20180201')\n",
    "download(1000,'/news__data23.csv','20180101')\n",
    "download(1000,'/news__data24.csv','20171201')\n",
    "download(1000,'/news__data25.csv','20171101')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
