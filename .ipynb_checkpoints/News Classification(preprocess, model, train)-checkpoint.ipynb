{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 전처리(Preprocess), Vocab생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Make Vocab ...\n",
      "Vocab Size : 60000\n",
      "Saving Vocab to /vocab.json...\n",
      "train processing...\n",
      "test processing...\n",
      "#Train data : 139879, #Test data : 6097\n"
     ]
    }
   ],
   "source": [
    "#-*- coding:utf-8 -*-\n",
    "\n",
    "from eunjeon import Mecab\n",
    "from collections import Counter\n",
    "import json\n",
    "\n",
    "tagger = Mecab() #형태소분석기\n",
    "\n",
    "def read_txt(path_to_file):\n",
    "    txt_ls = []\n",
    "    label_ls = []\n",
    "\n",
    "    with open(path_to_file) as f:\n",
    "        for i, line in enumerate(f.readlines()[1:]):\n",
    "            label= line[:1]\n",
    "            if label == '\"':\n",
    "                continue\n",
    "                \n",
    "            if len(line)>301: #gpu성능때문에 길이를 조정함, 성능에따라 조절하기\n",
    "                txt = line[2:300]    \n",
    "            elif len(line)>151:\n",
    "                txt = line[2:150]\n",
    "            else:\n",
    "                txt = line[2:30]\n",
    "\n",
    "            txt_ls.append(txt)\n",
    "            label_ls.append(int(label))\n",
    "    return txt_ls, label_ls\n",
    "\n",
    "def remove_empty_review(X, Y):   # 비어있는 문장 제거\n",
    "    empty_idx_ls = []\n",
    "\n",
    "    for idx, review in enumerate(X):\n",
    "        if len(review) == 0:\n",
    "            empty_idx_ls.append(idx)\n",
    "\n",
    "    empty_idx_ls = sorted(empty_idx_ls, reverse=True)\n",
    "\n",
    "    for empty_idx in empty_idx_ls:\n",
    "        del X[empty_idx], Y[empty_idx]\n",
    "\n",
    "    return X, Y\n",
    "\n",
    "def text2pos(text) : # 문장을 형태소 분석 토큰의 리스트로 변환\n",
    "    rep_list = []\n",
    "    for word, pos in tagger.pos(text) :\n",
    "        rep = '{}/{}'.format(word, pos)\n",
    "        rep_list.append(rep)\n",
    "\n",
    "    return rep_list\n",
    "\n",
    "def make_vocab(save_path, vocab_size = 60000) :   # Vocabulary 생성 \n",
    "    print('Make Vocab ...')\n",
    "    vocab = {'<PAD>':0, '<UNK>':1}\n",
    "    x_train, _ = read_txt('/rate_train.txt') # Vocab은 학습데이터로만 구성!\n",
    "    rep_counter = Counter()\n",
    "    for text in x_train :\n",
    "        for rep in text2pos(text) : \n",
    "            rep_counter[rep] += 1 # '단어/형태소' 태그 카운트\n",
    "\n",
    "    of = open('/rep2cnt.txt', 'w')\n",
    "    for rep, cnt in rep_counter.most_common() : # 가장 높은 빈도부터 순서대로\n",
    "        if(len(vocab) < vocab_size) : # vocab_size가 넘지 않도록\n",
    "            vocab[rep] = len(vocab)\n",
    "        of.write('{}\\t{}\\n'.format(rep, cnt))\n",
    "\n",
    "    print('Vocab Size : {}'.format(len(vocab)))\n",
    "    print('Saving Vocab to {}...'.format(save_path))\n",
    "    with open(save_path, 'w') as outfile:\n",
    "        json.dump(vocab, outfile)\n",
    "    # JSON >> dictionary를 저장할 수 있는 라이브러리\n",
    "\n",
    "def convert_token_to_idx(vocab, tokens): # token 리스트를 index로 변환 (즉, 단어 표현을 index로 변환)\n",
    "    idx = []\n",
    "    for token in tokens :\n",
    "        if(token not in vocab) : # vocab에 존재하지 않는 단어는 '<UNK>' index로!\n",
    "            idx.append(vocab['<UNK>'])\n",
    "        else :\n",
    "            idx.append(vocab[token])\n",
    "    return idx\n",
    "\n",
    "def make_dataset(save_path, vocab_path) :\n",
    "    # 데이터셋 생성\n",
    "    with open(vocab_path) as json_file :\n",
    "        vocab = json.load(json_file) # Vocab 불러오기\n",
    "\n",
    "    all_data = {'train' : [], 'test' : []}\n",
    "    for setname in ['train', 'test'] :\n",
    "        print('{} processing...'.format(setname))\n",
    "        text_list, answer_list = read_txt('/rate_{}.txt'.format(setname))\n",
    "        text_list, answer_list = remove_empty_review(text_list, answer_list)\n",
    "\n",
    "        for text, answer in zip(text_list, answer_list) :\n",
    "            tokens = text2pos(text)\n",
    "            idx = convert_token_to_idx(vocab, tokens)\n",
    "            \n",
    "            all_data[setname].append((text, tokens, idx, answer))\n",
    "\n",
    "    print('#Train data : {}, #Test data : {}'.format(len(all_data['train']), len(all_data['test'])))\n",
    "    with open(save_path, 'w') as outfile :\n",
    "        json.dump(all_data, outfile)\n",
    "\n",
    "if __name__ =='__main__' :\n",
    "    vocab_path = '/vocab.json' # vocab 경로\n",
    "    dataset_path = '/dataset.json' # dataset 경로\n",
    "    make_vocab(vocab_path)\n",
    "    make_dataset(dataset_path, vocab_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model (모델)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "\n",
    "class NewsClassification(nn.Module):\n",
    "    def __init__(self, opts, vocab_size, padding_index=0) :\n",
    "        super(NewsClassification, self).__init__()\n",
    "        # 1) embedding\n",
    "        embed_size = opts['embed_size']\n",
    "        hidden_size = opts['hidden_size']\n",
    "        lstm_layers = opts['n_layers']\n",
    "        self.embed = nn.Embedding(\n",
    "            num_embeddings=vocab_size,\n",
    "            embedding_dim=embed_size,\n",
    "            padding_idx=padding_index\n",
    "        )\n",
    "\n",
    "        # 2) LSTM\n",
    "        self.dropout = opts['dropout']\n",
    "        n_category = opts['n_category']\n",
    "        bidirectional = opts['bidirectional'] # 양방향이면 True, 단방향이면 False\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, lstm_layers, batch_first=True, bidirectional=bidirectional)\n",
    "        if(bidirectional) : # 양방향의 경우, 왼쪽, 오른쪽 방향 2개이기 때문에 출력값의 크기가 2배\n",
    "            input_size = 2 * hidden_size \n",
    "        else :\n",
    "            input_size = hidden_size\n",
    "\n",
    "        # 3) Linear\n",
    "        self.linear = nn.Linear(input_size, n_category) #fully connected layer\n",
    "        \n",
    "    def attention_net(self, lstm_output, final_state): # lstm_output - (batch size, maxlen, hidden*2), final_state - (1, batch size, hidden*2)\n",
    "        hidden = final_state.squeeze(0) # (batch size, hidden*2)\n",
    "        attn_weights = torch.bmm(lstm_output, hidden.unsqueeze(2)).squeeze(2) # (batch size, maxlen)\n",
    "        soft_attn_weights = F.softmax(attn_weights, 1)\n",
    "        new_hidden_state = torch.bmm(lstm_output.transpose(1, 2), soft_attn_weights.unsqueeze(2)).squeeze(2) # (batch size, hidden*2)\n",
    "\n",
    "        return new_hidden_state\n",
    "    \n",
    "    def forward(self, sent_tensor, sent_mask):\n",
    "        sent_emb = self.embed(sent_tensor)\n",
    "        output, _ = self.lstm(sent_emb)  # output - (batch size, maxlen, hidden*2)\n",
    "        \n",
    "        attn_output = self.attention_net(output, output.transpose(0, 1)[-1])\n",
    "        \n",
    "        #어텐션 적용했을때\n",
    "        logits = self.linear(attn_output)\n",
    "        \n",
    "        #안했을때\n",
    "        #logits = self.linear(output.transpose(0, 1)[-1]) \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습 (Train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-*- coding:utf-8 -*-\n",
    "import random\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "opts = {\n",
    "    # model\n",
    "    'embed_size' : 200,     # embedding 차원 (단어 -> Vector로 변환할 때 Vector의 크기!)\n",
    "    'hidden_size' : 250,    # hidden 차원 (중간 연산과정의 Vector 크기)\n",
    "    'n_layers' : 2,         # LSTM layer의 개수\n",
    "    'dropout' : 0.6,        # dropout rate\n",
    "    'bidirectional' : True, # 양방향 LSTM\n",
    "    'n_category' : 6,       # 6가지로 분류\n",
    "\n",
    "    # train\n",
    "    'vocab_path' : '/vocab.json',   # Vocab 경로\n",
    "    'data_path' : '/dataset.json',  # 데이터셋 경로\n",
    "    'use_gpu' : True,                   # GPU 사용여부. False면, CPU로 학습\n",
    "    'epochs' : 50,               # 학습횟수       \n",
    "    'batch_size' : 5,           # 배치크기\n",
    "    'learning_rate' : 0.001      # 학습률\n",
    "}\n",
    "\n",
    "class BatchGen: # Batch를 만들어 주는 Class\n",
    "    def __init__(self, raw_data, batch_size, evaluation=False):\n",
    "        self.batch_size = batch_size\n",
    "        self.eval = evaluation\n",
    "        data = raw_data\n",
    "        # shuffle\n",
    "        if not evaluation: # 학습데이터는 순서를 랜덤으로 Shuffle, 평가데이터는 X\n",
    "            indices = list(range(len(data)))\n",
    "            random.shuffle(indices)\n",
    "            data = [data[i] for i in indices]\n",
    "        data = [data[i:i + batch_size] for i in range(0, len(data), batch_size)]\n",
    "        # batch size만큼 data를 나눔 [ [batch1], [batch2], [batch3], ...]\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) # batch 개수 >> 'len(batch_data)'로 사용!\n",
    "\n",
    "    def __iter__(self):\n",
    "        for batch in self.data: \n",
    "            #(0, 1, 2, 3), (text, tokens, idx, answer)\n",
    "            text_list = [b[0] for b in batch]\n",
    "            token_list = [b[1] for b in batch]\n",
    "            batch_size = len(batch)\n",
    "            max_len = max([len(b[2]) for b in batch]) # batch의 문장 중에 가장 긴 문장의 길이\n",
    "            answer_tensor = torch.LongTensor([b[3] for b in batch]) # size : (batch_size)\n",
    "            sentence_tensor = torch.LongTensor(batch_size, max_len).fill_(0) # size : (batch_size, max_len)\n",
    "                                                                            # 0(PAD index)으로 초기화\n",
    "            for b_idx in range(batch_size) :\n",
    "                sent = batch[b_idx][2]\n",
    "                sentence_tensor[b_idx][:len(sent)] = torch.LongTensor(sent)\n",
    "                # sentence의 앞에서부터 채워넣음!\n",
    "                # 예를 들어 batch_size가 4이고, max_len이 5이면,\n",
    "                # 1,  2,  3,  4,  5,  >> 가장 긴 문장의 index\n",
    "                # 6,  0,  0,  0,  0,  >> 0은 padding index\n",
    "                # 7,  8,  9,  0,  0\n",
    "                # 10, 11, 12, 13, 0\n",
    "                # 14, 15, 0,  0,  0\n",
    "                # 와 같은 tensor(행렬)이 생성\n",
    "\n",
    "            sentence_mask = sentence_tensor.eq(0) # 0인 부분만 1\n",
    "\n",
    "            yield [text_list, token_list, sentence_tensor, sentence_mask, answer_tensor]\n",
    "\n",
    "def load_data(vocab_path, data_path) :\n",
    "    with open(vocab_path) as json_file :\n",
    "        vocab = json.load(json_file)\n",
    "    with open(data_path) as json_file :\n",
    "        all_data = json.load(json_file)\n",
    "\n",
    "    return len(vocab), all_data['train'], all_data['test']\n",
    "\n",
    "\n",
    "if __name__ == '__main__' :\n",
    "    print('[Program Start...]')\n",
    "    print(opts)\n",
    "    vocab_size, train_data, test_data = load_data(opts['vocab_path'], opts['data_path'])\n",
    "    \n",
    "\n",
    "    model = NewsClassification(opts, vocab_size) # 모델\n",
    "    if(opts['use_gpu']) : # gpu 사용한다면,\n",
    "        model = model.cuda()\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), opts['learning_rate'])\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    best_acc = 0.\n",
    "    for epoch in range(opts['epochs']) :\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_batch = BatchGen(train_data, opts['batch_size']) # 학습 batch\n",
    "\n",
    "        \n",
    "        \n",
    "        for batch in train_batch :\n",
    "            sent_tensor = Variable(batch[2]) # loss 계산을 위한 Variable\n",
    "            sent_mask = Variable(batch[3])\n",
    "            ans_tensor = Variable(batch[4])\n",
    "            if(opts['use_gpu']) : # gpu 사용한다면,\n",
    "                sent_tensor = sent_tensor.cuda()\n",
    "                sent_mask = sent_mask.cuda()\n",
    "                ans_tensor = ans_tensor.cuda()\n",
    "\n",
    "            logits = model.forward(sent_tensor, sent_mask)\n",
    "\n",
    "            loss = criterion(logits, ans_tensor) # loss 계산\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        print('Epoch : {}, Average Loss : {}'.format(epoch+1, train_loss/len(train_batch)))\n",
    "\n",
    "        test_batch = BatchGen(test_data, opts['batch_size'], evaluation=True) # 평가 batch\n",
    "        model.eval()\n",
    "        predict_list = []\n",
    "        n_correct = 0. # 정답 개수 Count\n",
    "        for batch in test_batch :\n",
    "            sent_tensor = Variable(batch[2])\n",
    "            sent_mask = Variable(batch[3])\n",
    "            ans_tensor = Variable(batch[4])\n",
    "            if(opts['use_gpu']) :\n",
    "                sent_tensor = sent_tensor.cuda()\n",
    "                sent_mask = sent_mask.cuda()\n",
    "                ans_tensor = ans_tensor.cuda()\n",
    "\n",
    "            logits = model.forward(sent_tensor, sent_mask)\n",
    "            predict = F.softmax(logits, dim=1).argmax(dim=1) # 가장 큰 index 선택\n",
    "\n",
    "            n_correct += (predict == ans_tensor.long()).sum().item() \n",
    "\n",
    "        acc = n_correct / len(test_data) * 100.\n",
    "\n",
    "        if(acc > best_acc) :\n",
    "            best_acc = acc\n",
    "            print('Test Accuracy : {} [new best acc]'.format(acc))\n",
    "        else :\n",
    "            print('Test Accuracy : {}'.format(acc))\n",
    "        \n",
    "        # 학습이 끝난 신경망 모델 저장\n",
    "        #params = model.state_dict()  \n",
    "        #torch.save(params, \"model_\" + str(epoch) + \".prm\", pickle_protocol = 4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
