{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "\n",
    "#모델 \n",
    "\n",
    "\n",
    "class NewsClassification(nn.Module):\n",
    "    def __init__(self, opts, vocab_size, padding_index=0) :\n",
    "        super(NewsClassification, self).__init__()\n",
    "        # 1) embedding\n",
    "        embed_size = opts['embed_size']\n",
    "        hidden_size = opts['hidden_size']\n",
    "        lstm_layers = opts['n_layers']\n",
    "        self.embed = nn.Embedding(\n",
    "            num_embeddings=vocab_size,\n",
    "            embedding_dim=embed_size,\n",
    "            padding_idx=padding_index\n",
    "        )\n",
    "\n",
    "        # 2) LSTM\n",
    "        self.dropout = opts['dropout']\n",
    "        n_category = opts['n_category']\n",
    "        bidirectional = opts['bidirectional'] # 양방향이면 True, 단방향이면 False\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, lstm_layers, batch_first=True, bidirectional=bidirectional)\n",
    "        if(bidirectional) : # 양방향의 경우, 왼쪽, 오른쪽 방향 2개이기 때문에 출력값의 크기가 2배\n",
    "            input_size = 2 * hidden_size \n",
    "        else :\n",
    "            input_size = hidden_size\n",
    "\n",
    "        # 3) Linear\n",
    "        self.linear = nn.Linear(input_size, n_category) #fully connected layer\n",
    "        \n",
    "    def attention_net(self, lstm_output, final_state):\n",
    "        hidden = final_state.squeeze(0)\n",
    "        attn_weights = torch.bmm(lstm_output, hidden.unsqueeze(2)).squeeze(2)\n",
    "        soft_attn_weights = F.softmax(attn_weights, 1)\n",
    "        new_hidden_state = torch.bmm(lstm_output.transpose(1, 2), soft_attn_weights.unsqueeze(2)).squeeze(2)\n",
    "\n",
    "        return new_hidden_state\n",
    "    \n",
    "    def forward(self, sent_tensor, sent_mask):\n",
    "        sent_emb = self.embed(sent_tensor)\n",
    "        output, _ = self.lstm(sent_emb)\n",
    "\n",
    "        attn_output = self.attention_net(output, output.transpose(0, 1)[-1]) \n",
    "        logits = self.linear(attn_output)\n",
    "        return logits\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-*- coding:utf-8 -*-\n",
    "import random\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "#from model import NewsClassification\n",
    "import torch.nn.functional as F\n",
    "\n",
    "opts = {\n",
    "    # model\n",
    "    'embed_size' : 200,     # embedding 차원 (단어 -> Vector로 변환할 때 Vector의 크기!)\n",
    "    'hidden_size' : 250,    # hidden 차원 (중간 연산과정의 Vector 크기)\n",
    "    'n_layers' : 2,         # LSTM layer의 개수\n",
    "    'dropout' : 0.15,        # dropout rate (* 이건 직접 추가해보세용, 성능 비교)\n",
    "    'bidirectional' : True, # 양방향 LSTM\n",
    "    'n_category' : 6,       # 긍정, 부정 >> 2개\n",
    "\n",
    "    # train\n",
    "    'vocab_path' : '/vocab.json',   # Vocab 경로\n",
    "    'data_path' : '/dataset.json',  # 데이터셋 경로\n",
    "    'use_gpu' : True,                   # GPU 사용여부. False면, CPU로 학습\n",
    "    'epochs' : 50,                      \n",
    "    'batch_size' : 5,\n",
    "    'learning_rate' : 0.001\n",
    "}\n",
    "\n",
    "class BatchGen: # Batch를 만들어 주는 Class\n",
    "    def __init__(self, raw_data, batch_size, evaluation=False):\n",
    "        self.batch_size = batch_size\n",
    "        self.eval = evaluation\n",
    "        data = raw_data\n",
    "        # shuffle\n",
    "        if not evaluation: # 학습데이터는 순서를 랜덤으로 Shuffle, 평가데이터는 X\n",
    "            indices = list(range(len(data)))\n",
    "            random.shuffle(indices)\n",
    "            data = [data[i] for i in indices]\n",
    "        # chunk into batches\n",
    "        data = [data[i:i + batch_size] for i in range(0, len(data), batch_size)]\n",
    "        # batch size만큼 data를 나눔 [ [batch1], [batch2], [batch3], ...]\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) # batch 개수 >> 'len(batch_data)'로 사용!\n",
    "\n",
    "    def __iter__(self):\n",
    "        for batch in self.data: \n",
    "            #(0, 1, 2, 3), (text, tokens, idx, answer)\n",
    "            text_list = [b[0] for b in batch]\n",
    "            token_list = [b[1] for b in batch]\n",
    "            batch_size = len(batch)\n",
    "            max_len = max([len(b[2]) for b in batch]) # batch의 문장 중에 가장 긴 문장의 길이\n",
    "            answer_tensor = torch.LongTensor([b[3] for b in batch]) # size : (batch_size)\n",
    "            sentence_tensor = torch.LongTensor(batch_size, max_len).fill_(0) # size : (batch_size, max_len)\n",
    "                                                                            # 0(PAD index)으로 초기화\n",
    "            for b_idx in range(batch_size) :\n",
    "                sent = batch[b_idx][2]\n",
    "                sentence_tensor[b_idx][:len(sent)] = torch.LongTensor(sent)\n",
    "                # sentence의 앞에서부터 채워넣음!\n",
    "                # 예를 들어 batch_size가 4이고, max_len이 5이면,\n",
    "                # 1,  2,  3,  4,  5,  >> 가장 긴 문장의 index\n",
    "                # 6,  0,  0,  0,  0,  >> 0은 padding index\n",
    "                # 7,  8,  9,  0,  0\n",
    "                # 10, 11, 12, 13, 0\n",
    "                # 14, 15, 0,  0,  0\n",
    "                # 와 같은 tensor(행렬)이 생성\n",
    "\n",
    "\n",
    "            sentence_mask = sentence_tensor.eq(0) # 0인 부분만 1\n",
    "\n",
    "            yield [text_list, token_list, sentence_tensor, sentence_mask, answer_tensor]\n",
    "\n",
    "def load_data(vocab_path) :\n",
    "    with open(vocab_path) as json_file :\n",
    "        vocab = json.load(json_file)\n",
    "\n",
    "    return len(vocab)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력문장 >> 텐서로 변환\n",
    "\n",
    "#-*- coding:utf-8 -*-\n",
    "\n",
    "#from konlpy.tag import Okt#,Hannanum, Kkma, Komoran, ...\n",
    "from eunjeon import Mecab\n",
    "from collections import Counter\n",
    "import json\n",
    "\n",
    "tagger = Mecab()\n",
    "\n",
    "def read_txt(input):\n",
    "    txt_ls = []\n",
    "    label_ls = []\n",
    "\n",
    "    txt_ls.append(input)\n",
    "    label_ls.append(0)\n",
    "    \n",
    "    return txt_ls, label_ls\n",
    "\n",
    "def remove_empty_review(X, Y):\n",
    "    # 비어있는 문장 제거 (예제와 동일)\n",
    "    empty_idx_ls = []\n",
    "\n",
    "    for idx, review in enumerate(X):\n",
    "        if len(review) == 0:\n",
    "            empty_idx_ls.append(idx)\n",
    "\n",
    "    empty_idx_ls = sorted(empty_idx_ls, reverse=True)\n",
    "\n",
    "    for empty_idx in empty_idx_ls:\n",
    "        del X[empty_idx], Y[empty_idx]\n",
    "\n",
    "    return X, Y\n",
    "\n",
    "def text2pos(text) :\n",
    "    # 문장을 형태소 분석 토큰의 리스트로 변환\n",
    "    rep_list = []\n",
    "    for word, pos in tagger.pos(text) :\n",
    "        rep = '{}/{}'.format(word, pos)\n",
    "        rep_list.append(rep)\n",
    "\n",
    "    return rep_list\n",
    "\n",
    "\n",
    "\n",
    "def convert_token_to_idx(vocab, tokens):\n",
    "    # token 리스트를 index로 변환 (즉, 단어 표현을 index로 변환)\n",
    "    idx = []\n",
    "    for token in tokens :\n",
    "        if(token not in vocab) : # vocab에 존재하지 않는 단어는 '<UNK>' index로!\n",
    "            idx.append(vocab['<UNK>'])\n",
    "        else :\n",
    "            idx.append(vocab[token])\n",
    "    return idx\n",
    "\n",
    "vocab_path = '/vocab.json' # vocab 경로\n",
    "with open(vocab_path) as json_file :\n",
    "    vocab = json.load(json_file) # Vocab 불러오기\n",
    "\n",
    "vocab_size = load_data(opts['vocab_path'])\n",
    "\n",
    "model = NewsClassification(opts, vocab_size) # 모델\n",
    "\n",
    "prm = torch.load(\"84.2.prm\", map_location=\"cpu\")\n",
    "model.load_state_dict(prm)\n",
    "model.eval()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), opts['learning_rate'])\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import zmq\n",
    "\n",
    "context = zmq.Context()\n",
    "socket = context.socket(zmq.REP)\n",
    "socket.bind(\"tcp://127.0.0.1:5858\")\n",
    "\n",
    "while True:\n",
    "    message = socket.recv()\n",
    "    test = message.decode('utf-8')\n",
    "\n",
    "    all_data = {'test' : []}\n",
    "    for setname in ['test'] :\n",
    "        text_list, answer_list = read_txt(test)\n",
    "        text_list, answer_list = remove_empty_review(text_list, answer_list)\n",
    "\n",
    "        for text, answer in zip(text_list, answer_list) :\n",
    "            tokens = text2pos(text)\n",
    "            idx = convert_token_to_idx(vocab, tokens)\n",
    "\n",
    "            all_data[setname].append((text, tokens, idx, answer))\n",
    "            all_data[setname].append((text, tokens, idx, answer))\n",
    "\n",
    "\n",
    "\n",
    "    test_data = all_data['test']\n",
    "\n",
    "    test_batch = BatchGen(test_data, opts['batch_size'], evaluation=True)\n",
    "    #  attention(모델)부분에서 차원축소과정에서 에러뜨는 거 같아서 배치size 2개(입력받은거 두개로해서) 테스트 진행하였음\n",
    "\n",
    "    predict_list = []\n",
    "\n",
    "    for batch in test_batch :\n",
    "        sent_tensor = Variable(batch[2])\n",
    "        sent_mask = Variable(batch[3])\n",
    "        ans_tensor = Variable(batch[4])\n",
    "\n",
    "        logits = model.forward(sent_tensor, sent_mask)\n",
    "\n",
    "        predict = F.softmax(logits, dim=1).argmax(dim=1)\n",
    "\n",
    "        predict = predict.tolist()\n",
    "        predict = predict[0]\n",
    "\n",
    "        if predict == 0:\n",
    "            test = \"정치\"\n",
    "        elif predict == 1:\n",
    "            test = \"경제\"\n",
    "        elif predict == 2:\n",
    "            test = \"사회\"\n",
    "        elif predict == 3:\n",
    "            test = \"생활/문화\"\n",
    "        elif predict == 4:\n",
    "            test = \"세계\"\n",
    "        elif predict == 5:\n",
    "            test = \"IT/과학\"\n",
    "        print(test)\n",
    "    socket.send(test.encode('utf-8')) #//클라이언트쪽으로 보내는 문구"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
